# Learning to Complement Humans

- link

[Learning to Complement Humans](https://arxiv.org/abs/2005.00582)

## 背景&目的：なぜその問題を解決したいのか、どのように解決したのか？

機械学習を用いて人を支援するシステムをこれまで、モデルのみのパフォーマンスに注目されてきた。
これは、人を支援するという視点から人の専門性を無視していることになる。
この論文では、人と機械のパフォーマンスを向上させるようにモデルを学習させる手法を提案する。
具体的にモデルが学習するのは、機械学習モデルが予測をするのか人に予測を依頼するのかを判断すること。

## 提案：解決に向けたキーアイデアは何か

### 問題の定式化

定式化する時に解くタスクは教師あり学習。
特徴量$x\in \mathcal{X}$から正解ラベル$y\in \mathcal{Y}$を予測する。
この時、予測に使うモデル$m$はパラメータ$\theta$をもち、予測$\hat{y} = m_{\theta}(x)$を出力する。

学習に使うデータは人によるラベル$h\in \mathcal{Y}$を含む$\{(x,h,y)_1^N\}\sim P$になっている。
ここで$P$はなんらかの分布を表している。

モデルは最初に、特徴$x$に対してコスト$c$をかけて特徴$h$を入手するかどうかを決定する。
この時の$q_{\theta}(x)$が1の時人に問い合わせ、0の時問い合わせを行わない。
チームの効用は$u(y, \hat{y})$で計算され、人に問い合わせを行った時、この関数の値からコスト$c$を引く。
これを定式化すると以下の式を最大化する。
$$\mathbb{E}_{(x,y,h)\sim P}\left[q_{theta}(x)(u(y, m_{\theta}(x,h)0-c))+(1-q_{\theta}(x))(u(y,m_{\theta}(x)))\right] \tag{1}$$

### Fixed Discriminative Approach

モデル$m$をタスクに対して最適化する。
この時、人に対して問い合わせを行うことは考えない。
$\mathbb{E}_{(x,y)\sim P}[u(y, m_{\theta}(x))]$を最大化するように学習を行う。

モデル$m$に対して学習が終わった後、$q$について問い合わせを判断させる分類器を学習させる。
この時式(1)を最大化するように学習を行う。

### Joint Discriminative Approach

式1の期待値を微分可能な形で定式化し直す。
学習中は$q_{\theta}(x)$は離散値ではなく、連続値をとるように設定する.

$$q_{\theta}(x)l(y,m_{\theta}(x,h))+(1-q_{\theta}(x))l(y, m_{\theta}(x)) + c q_{\theta}(x)\tag{2}$$

この時の$l$はなんらかの損失関数で、効用関数の代わりに用いている。
実験用いる目的関数は式3のものを用いる。
$$l(y ,q_{\theta}(x)h+(1-q_{\theta}(x))m_{\theta}(x)) + cq_{\theta}(x)\tag{3}$$
ここで$m_{\theta}(x,h)$を$h$で近似している理由は、実験によってパフォーマンスの差が見られなかったから。

この学習において人に対して問い合わせを行う基準は式4のものを用いる。
この式は、モデルの確信度$max(m_{\theta}(x))$を考慮した条件式になっている。

$$(1-q_{\theta}(x))max(m_{\theta}(x))<q_{\theta}(x)\tag{4}$$

### Fixed Value of Infomation Approach

このアプローチでは3つの確率モデルを最適化する。
$p_{\alpha}(y|x)$は与えられた特徴からラベルの確率分布を計算する。
$p_{\beta}(h|x)$は与えられた特徴に対する人の反応（ラベル$h$のこと）を確率分布を計算する。
$p_{\gamma}(y|h, x)$は特徴と人の反応がある時の確率分布を計算する。
$\alpha, \beta, \gamma$がそれぞれの確率モデルのパラメータに対応する。

モデルの学習は個別に行われる。

最初に人に問い合わせを行わない時の効用を式5で推定する。
$$u_{nq}=max_{\hat{y}\in \mathcal{Y}}\left(\sum_{y\in\mathcal{Y}}p_{\alpha}(y|x)u(\hat{y}, y)\right)\tag{5}$$

ここで、人に問い合わせる前に$h$を入手することはできないため、$p_{\beta}(h|x)$からサンプルを取得して値を推定する。
計算式は式6の通り。
$$u_q=\mathbb{E}_{h\sim p_{\beta}(h|x)}\left[max_{\hat{y}\in \mathcal{Y}}\left(\sum_{y\in \mathcal{Y}}p_{\gamma}(y|x,h)u(\hat{y}, y)\right)\right]\tag{6}$$

ここで、$u_q > u_{nq}$の時、人に問い合わせを行う。

### Joint Value of Information Approach

$p_{\alpha},p_{\beta},p_{\gamma}$を同時に学習させるために、NNで実装する。
アルゴリズムはこう。
<img src="img/Screen%20Shot%202022-08-05%20at%2011.17.57.png" width="60%">
これで学習させ、$u_q > u_{nq}$の時、人に問い合わせを行う。

## 結果：結局問題は解決されたのか．新しくわかったことは？

### 実験

使用するデータセットはGalaxy Zoo ProjectとCAMELYON16の２種類を用いて実験を行う。

図２には、全体の損失の推移（分類誤差＋コストの量）が示されている。
jointモデルの方が、fixedモデルよりも良い結果を出している（グラフのプロットが下の方になっているから）。
また、VOIを考慮した手法の方がよい結果になっていることがわかる。
全て人に依頼する時よりも、モデルを使った方がコストパフォーマンスのバランスが良くなっている。

<img src="img/Screen%20Shot%202022-08-05%20at%2011.38.08.png" width="90%">
